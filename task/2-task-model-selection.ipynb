{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2e8e08d-78ed-4419-b223-26946a1cc48e",
   "metadata": {},
   "source": [
    "# Model Selection in Machine Learning: Predicting Housing Prices\n",
    "We'll explore how different models perform on a simple task: predicting housing prices using the Boston Housing dataset. This dataset is built into Scikit-Learn, making it easily accessible for our exercises. Let's get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fbf86f-5475-4c72-9904-5818db6aaaa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda5eda6-7973-45cc-a661-d61ddd97c3e8",
   "metadata": {},
   "source": [
    "## Load and Explore the Dataset\r\n",
    "The California Housing dataset contains metrics such as the median income, housing median age, average rooms, average bedrooms, population, average occupancy, latitude, and longitude for various blocks in California."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ebe8d59-90b4-489c-a5ca-611ecb5b793b",
   "metadata": {},
   "outputs": [],
   "source": [
    "california = fetch_california_housing()\n",
    "X = pd.DataFrame(california.data, columns=california.feature_names)\n",
    "y = california.target\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc57c8d7-7779-40d9-a4f7-f3d67e306030",
   "metadata": {},
   "source": [
    "## Visualizing the Data\r",
    "A quick visualization to understand our data better.\r\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66577fcf-03dc-473c-9c90-56b6101ea3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X['MedInc'], y)\n",
    "plt.xlabel('Median Income (tens of thousands)')\n",
    "plt.ylabel('Median House Value ($100K)')\n",
    "plt.title('Income vs. House Value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f39ddbd-8e30-4375-9645-3ed98c454074",
   "metadata": {},
   "source": [
    "## Splitting the Dataset\r",
    "Split the dataset into a training set and a test set to evaluate our models effectively.\r\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0aeae1f-aff7-4f8a-8b2a-acab5ef21048",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78f5c5a-6738-4bc5-8365-5bb58df8a4b5",
   "metadata": {},
   "source": [
    "## Model Selection\n",
    "We will explore three different models to predict housing prices: Linear Regression, Decision Tree Regressor, and Random Forest Regressor.\r\n",
    "#### How do we evaluate the model?\n",
    "We will be using MAE.</br>\n",
    "MAE stands for Mean Absolute Error. It's a way to measure how close your machine learning model's predictions are to the actual outcomes. Here's a simple way to understand it:\r\n",
    "\r\n",
    "Imagine you're trying to guess the ages of several people. After making your guesses, you find out their real ages and calculate how far off you were for each person. Some guesses might be too high, and some might be too low, but you're only interested in how wrong you were, regardless of the direction. So, you take the absolute value of each mistake (which turns any negative numbers into positives) and then average these to get a single number that tells you how well you did overall.\r\n",
    "\r\n",
    "The lower the MAE, the closer your guesses were to the real ages, which means your predictions were pretty accurate!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a587291-7ccb-4ef7-b764-dad31a207d7d",
   "metadata": {},
   "source": [
    "### 1. Linear Regression\r",
    "A good baseline model due to its simplicity.\r\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3de48d1-9b0c-48f6-b116-6fe9d5846a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Linear Regression model\n",
    "linear_model = LinearRegression()\n",
    "linear_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "linear_predictions = linear_model.predict(X_test)\n",
    "linear_mae = mean_absolute_error(y_test, linear_predictions)\n",
    "print(\"Linear Regression MAE:\", linear_mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1efe9a-f707-45fd-b81d-bd3c4ce7d26d",
   "metadata": {},
   "source": [
    "### 2. Decision Tree Regressor\r\n",
    "Useful for capturing non-linear relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06f1437-1023-449d-b64f-7c2ec2a7f35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Decision Tree model\n",
    "tree_model = DecisionTreeRegressor(random_state=42)\n",
    "tree_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "tree_predictions = tree_model.predict(X_test)\n",
    "tree_mae = mean_absolute_error(y_test, tree_predictions)\n",
    "print(\"Decision Tree Regressor MAE:\", tree_mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b729e45b-04a1-441c-a354-3f484ae516c0",
   "metadata": {},
   "source": [
    "### 3. Random Forest Regressor\n",
    "An ensemble method that generally provides high accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541c2d47-fbf3-4f3a-b305-09730e402d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Random Forest model\n",
    "forest_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "forest_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "forest_predictions = forest_model.predict(X_test)\n",
    "forest_mae = mean_absolute_error(y_test, forest_predictions)\n",
    "print(\"Random Forest Regressor MAE:\", forest_mae)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b67cc2-769d-484b-88f7-892f8db8c118",
   "metadata": {},
   "source": [
    "## Comparison\r",
    "After training and evaluating our models, let's compare their performance.\r\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e074fa9e-8841-41c3-9391-17625d071af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparing the MAE of all models\n",
    "mae_values = [linear_mae, tree_mae, forest_mae]\n",
    "model_names = ['Linear Regression', 'Decision Tree', 'Random Forest']\n",
    "\n",
    "plt.bar(model_names, mae_values)\n",
    "plt.ylabel('Mean Absolute Error')\n",
    "plt.title('Model Comparison')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed3e7cd-4291-44e3-9ee3-9acc2aaf90c1",
   "metadata": {},
   "source": [
    "## Improve\n",
    "We saw some base models and compared them, but we can improve the performance of our model by modifying parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851f343d-7d75-41f2-b7e1-af4569da9b41",
   "metadata": {},
   "source": [
    "### Decision Tree Regressor Parameters\r\n",
    "Let's adjust the max_depth parameter of the Decision Tree Regressor and see how it influences the model\n",
    "\n",
    "max_depth: Controls the maximum depth of the tree. A deeper tree can capture more complex patterns but also risks overfitting. Setting it too low might not capture enough complexity, leading to underfitting.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3adaa0-c9b4-4e19-a40f-8f4a204bb01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Varying the max_depth of the Decision Tree\n",
    "max_depth_values = [2, 4, 6, 8, None]  # None means the tree can grow as much as it needs\n",
    "dt_mae_scores = []\n",
    "\n",
    "for depth in max_depth_values:\n",
    "    dt_model = DecisionTreeRegressor(max_depth=depth, random_state=42)\n",
    "    dt_model.fit(X_train, y_train)\n",
    "    dt_predictions = dt_model.predict(X_test)\n",
    "    dt_mae = mean_absolute_error(y_test, dt_predictions)\n",
    "    dt_mae_scores.append(dt_mae)\n",
    "\n",
    "# Plotting the MAE scores for different max_depth values\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(['2', '4', '6', '8', 'None'], dt_mae_scores, marker='o')\n",
    "plt.xlabel('Max Depth')\n",
    "plt.ylabel('Mean Absolute Error')\n",
    "plt.title('Decision Tree Performance vs. Max Depth')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d8b589-7186-4bb0-a905-ef4abc6c2916",
   "metadata": {},
   "source": [
    "### Random Forest Regressor Parameters\r\n",
    "For the Random Forest Regressor, let's tweak the n_estimators parameter, which controls the number of trees in the forest\n",
    "\n",
    "n_estimators: Determines the number of trees in the forest. More trees can lead to better performance but also require more computational resources. It's a balance between performance and efficiency.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceba9c9e-da74-4351-a81e-f4ba9b818f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjusting the n_estimators of the Random Forest\n",
    "n_estimators_values = [10, 50, 100, 200]\n",
    "rf_mae_scores = []\n",
    "\n",
    "for n_estimators in n_estimators_values:\n",
    "    rf_model = RandomForestRegressor(n_estimators=n_estimators, random_state=42)\n",
    "    rf_model.fit(X_train, y_train)\n",
    "    rf_predictions = rf_model.predict(X_test)\n",
    "    rf_mae = mean_absolute_error(y_test, rf_predictions)\n",
    "    rf_mae_scores.append(rf_mae)\n",
    "\n",
    "# Plotting the MAE scores for different n_estimators values\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(n_estimators_values, rf_mae_scores, marker='o')\n",
    "plt.xlabel('Number of Trees (n_estimators)')\n",
    "plt.ylabel('Mean Absolute Error')\n",
    "plt.title('Random Forest Performance vs. Number of Trees')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55903c5b-d162-4ab7-801a-5f49b65f1bea",
   "metadata": {},
   "source": [
    "### Playing with Parameters\n",
    "Adjusting these parameters allows us to control the model's complexity and its ability to generalize from training data to unseen data. Here's how you can play with them:\n",
    "\n",
    "For the Decision Tree, start with a low max_depth and gradually increase it to see how the model's performance changes. Notice when the performance starts to degrade, indicating overfitting.</br>\n",
    "For the Random Forest, increasing n_estimators generally improves model performance up to a point. Identify the sweet spot where adding more trees has diminishing returns on performance improvement. </br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693552fa-8481-4184-808c-5103a1d73480",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
